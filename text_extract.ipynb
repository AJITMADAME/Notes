{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa778f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.7-cp39-none-win_amd64.whl (3.2 MB)\n",
      "     ---------------------------------------- 3.2/3.2 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting PyMuPDFb==1.24.6\n",
      "  Downloading PyMuPDFb-1.24.6-py3-none-win_amd64.whl (12.5 MB)\n",
      "     ---------------------------------------- 12.5/12.5 MB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.6 pymupdf-1.24.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pymupdf.exe is installed in 'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pymupdf.exe is installed in 'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271c5400",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13692\\575402743.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Convert the list of lists to a pandas DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Print the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(r\"C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf\")\n",
    "\n",
    "# Extract text from the first page\n",
    "page = pdf_document[0]\n",
    "text = page.get_text(\"text\")\n",
    "\n",
    "# Manually parsing the table from the text (for this example)\n",
    "lines = text.split('\\n')\n",
    "\n",
    "# Initialize an empty list to store the table data\n",
    "table_data = []\n",
    "\n",
    "# Define a flag to indicate if we're in the table section\n",
    "in_table = False\n",
    "\n",
    "# Process each line\n",
    "for line in lines:\n",
    "    if 'Disability' in line and 'Category' in line:\n",
    "        in_table = True\n",
    "        headers = line.split()\n",
    "        table_data.append(headers)\n",
    "        continue\n",
    "\n",
    "    if in_table:\n",
    "        if line.strip() == '':\n",
    "            continue\n",
    "        row = line.split()\n",
    "        table_data.append(row)\n",
    "\n",
    "# Convert the list of lists to a pandas DataFrame\n",
    "df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(r\"C:\\Users\\ASUS\\Desktop\\text_extract\\outputextracted_table.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e9d591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "Example table \n",
      "This is an example of a data table. \n",
      "Disability \n",
      "Category \n",
      "Participants \n",
      "Ballots \n",
      "Completed \n",
      "Ballots \n",
      "Incomplete/ \n",
      "Terminated \n",
      "Results \n",
      "Accuracy \n",
      "Time to \n",
      "complete \n",
      "Blind \n",
      "5 \n",
      "1 \n",
      "4 \n",
      "34.5%, n=1 \n",
      "1199 sec, n=1 \n",
      "Low Vision \n",
      "5 \n",
      "2 \n",
      "3 \n",
      "98.3% n=2 \n",
      "(97.7%, n=3) \n",
      "1716 sec, n=3 \n",
      "(1934 sec, n=2) \n",
      "Dexterity \n",
      "5 \n",
      "4 \n",
      "1 \n",
      "98.3%, n=4 \n",
      "1672.1 sec, n=4 \n",
      "Mobility \n",
      "3 \n",
      "3 \n",
      "0 \n",
      "95.4%, n=3 \n",
      "1416 sec, n=3 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(r\"C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf\")\n",
    "\n",
    "# Extract text from the first page\n",
    "page = pdf_document[0]\n",
    "text = page.get_text(\"text\")\n",
    "\n",
    "# Print the extracted text for debugging\n",
    "print(\"Extracted Text:\")\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6188123f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "Example table \n",
      "This is an example of a data table. \n",
      "Disability \n",
      "Category \n",
      "Participants \n",
      "Ballots \n",
      "Completed \n",
      "Ballots \n",
      "Incomplete/ \n",
      "Terminated \n",
      "Results \n",
      "Accuracy \n",
      "Time to \n",
      "complete \n",
      "Blind \n",
      "5 \n",
      "1 \n",
      "4 \n",
      "34.5%, n=1 \n",
      "1199 sec, n=1 \n",
      "Low Vision \n",
      "5 \n",
      "2 \n",
      "3 \n",
      "98.3% n=2 \n",
      "(97.7%, n=3) \n",
      "1716 sec, n=3 \n",
      "(1934 sec, n=2) \n",
      "Dexterity \n",
      "5 \n",
      "4 \n",
      "1 \n",
      "98.3%, n=4 \n",
      "1672.1 sec, n=4 \n",
      "Mobility \n",
      "3 \n",
      "3 \n",
      "0 \n",
      "95.4%, n=3 \n",
      "1416 sec, n=3 \n",
      " \n",
      "\n",
      "Extracted Table Data:\n",
      "No table data found.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_document = fitz.open(r\"C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf\")\n",
    "\n",
    "# Extract text from the first page\n",
    "page = pdf_document[0]\n",
    "text = page.get_text(\"text\")\n",
    "\n",
    "# Print the extracted text for debugging\n",
    "print(\"Extracted Text:\")\n",
    "print(text)\n",
    "\n",
    "# Initialize an empty list to store the table data\n",
    "table_data = []\n",
    "\n",
    "# Define a flag to indicate if we're in the table section\n",
    "in_table = False\n",
    "current_row = []\n",
    "\n",
    "# Process each line\n",
    "lines = text.split('\\n')\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    \n",
    "    if 'Disability Category' in line:\n",
    "        in_table = True\n",
    "        headers = line.split()\n",
    "        table_data.append(headers)\n",
    "        continue\n",
    "\n",
    "    if in_table:\n",
    "        # Check if this line is part of the same row\n",
    "        if len(line.split()) > 1:\n",
    "            if current_row:\n",
    "                table_data.append(current_row)\n",
    "            current_row = line.split()\n",
    "        else:\n",
    "            current_row.append(line)\n",
    "\n",
    "# Add the last row if it exists\n",
    "if current_row:\n",
    "    table_data.append(current_row)\n",
    "\n",
    "# Check the extracted table data for debugging\n",
    "print(\"Extracted Table Data:\")\n",
    "for row in table_data:\n",
    "    print(row)\n",
    "\n",
    "# Ensure we have extracted headers and at least one row of data\n",
    "if len(table_data) > 1:\n",
    "    # Convert the list of lists to a pandas DataFrame\n",
    "    df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(\"Extracted DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(r\"C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table.csv\", index=False)\n",
    "else:\n",
    "    print(\"No table data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bc2fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tabula-py\n",
      "  Downloading tabula_py-2.9.3-py3-none-any.whl (12.0 MB)\n",
      "     ---------------------------------------- 12.0/12.0 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from tabula-py) (1.21.5)\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tabula-py) (1.4.4)\n",
      "Collecting distro\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.25.3->tabula-py) (1.16.0)\n",
      "Installing collected packages: distro, tabula-py\n",
      "Successfully installed distro-1.9.0 tabula-py-2.9.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script distro.exe is installed in 'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install tabula-py\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4b24f86",
   "metadata": {},
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf\"\n",
    "\n",
    "# Extract tables from the PDF file\n",
    "tables = tabula.read_pdf(pdf_path, pages=\"all\")\n",
    "\n",
    "# Check if any tables were found\n",
    "if tables:\n",
    "    # Assuming the table we need is the first one found\n",
    "    df = tables[0]\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(\"Extracted DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table.csv\"\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "else:\n",
    "    print(\"No tables found in the PDF.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdc150b2",
   "metadata": {},
   "source": [
    "!pip install camelot-py[cv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be54f9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted DataFrame:\n",
      "                                                    0      1      2       3  \\\n",
      "0   Table name : daily historical stock prices & v...                         \n",
      "1                                                Date   Open   High     Low   \n",
      "2                                          01/04/2017  62.48  62.75   62.12   \n",
      "3                                          01/03/2017  62.79  62.84  62.125   \n",
      "4                                          12/30/2016  62.96  62.99   62.03   \n",
      "5                                          12/29/2016  62.86   63.2   62.73   \n",
      "6                                          12/28/2016   63.4   63.4   62.83   \n",
      "7                                          12/27/2016  63.21  64.07   63.21   \n",
      "8                                          12/23/2016  63.45  63.54    62.8   \n",
      "9                                          12/22/2016  63.84   64.1  63.405   \n",
      "10                                         12/21/2016  63.43   63.7   63.12   \n",
      "11                                         12/20/2016  63.69   63.8  63.025   \n",
      "12                                         12/19/2016  62.56  63.77   62.42   \n",
      "13                                         12/16/2016  62.95  62.95  62.115   \n",
      "\n",
      "               4           5  \n",
      "0                             \n",
      "1   Close / Last      Volume  \n",
      "2           62.3  21,325,140  \n",
      "3          62.58  20,655,190  \n",
      "4          62.14  25,575,720  \n",
      "5           62.9  10,248,460  \n",
      "6          62.99  14,348,340  \n",
      "7          63.28  11,743,650  \n",
      "8          63.24  12,399,540  \n",
      "9          63.55  22,175,270  \n",
      "10         63.54  17,084,370  \n",
      "11         63.54  26,017,470  \n",
      "12         63.62  34,318,500  \n",
      "13          62.3  42,452,660  \n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\sample1.pdf\"\n",
    "\n",
    "# Extract tables from the PDF file\n",
    "tables = camelot.read_pdf(pdf_path, pages='1', flavor='stream')\n",
    "\n",
    "# Check if any tables were found\n",
    "if tables:\n",
    "    # Assuming the table we need is the first one found\n",
    "    df = tables[0].df\n",
    "    df\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(\"Extracted DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table1.csv\"\n",
    "    df.to_csv(output_csv_path, index=False, header=False)\n",
    "else:\n",
    "    print(\"No tables found in the PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dcdad597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted DataFrame:\n",
      "1         Date   Open   High     Low Close / Last      Volume\n",
      "2   01/04/2017  62.48  62.75   62.12         62.3  21,325,140\n",
      "3   01/03/2017  62.79  62.84  62.125        62.58  20,655,190\n",
      "4   12/30/2016  62.96  62.99   62.03        62.14  25,575,720\n",
      "5   12/29/2016  62.86   63.2   62.73         62.9  10,248,460\n",
      "6   12/28/2016   63.4   63.4   62.83        62.99  14,348,340\n",
      "7   12/27/2016  63.21  64.07   63.21        63.28  11,743,650\n",
      "8   12/23/2016  63.45  63.54    62.8        63.24  12,399,540\n",
      "9   12/22/2016  63.84   64.1  63.405        63.55  22,175,270\n",
      "10  12/21/2016  63.43   63.7   63.12        63.54  17,084,370\n",
      "11  12/20/2016  63.69   63.8  63.025        63.54  26,017,470\n",
      "12  12/19/2016  62.56  63.77   62.42        63.62  34,318,500\n",
      "13  12/16/2016  62.95  62.95  62.115         62.3  42,452,660\n",
      "CSV file saved successfully to: C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table1.csv\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\sample1.pdf\"\n",
    "\n",
    "# Extract tables from the PDF file using 'stream' flavor\n",
    "tables = camelot.read_pdf(pdf_path, pages='1', flavor='stream')\n",
    "\n",
    "# Check if any tables were found\n",
    "if tables:\n",
    "    # Assuming the table we need is the first one found\n",
    "    df = tables[0].df\n",
    "    \n",
    "    # Adjust the headers properly\n",
    "    new_header = df.iloc[1]  # take the second row for headers\n",
    "    df = df[2:]  # take the data less the header row\n",
    "    df.columns = new_header  # set the header row as the df header\n",
    "\n",
    "    # Print the DataFrame to inspect\n",
    "    print(\"Extracted DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table1.csv\"\n",
    "    df.to_csv(output_csv_path, index=False)  # Ensure headers are included\n",
    "\n",
    "    print(f\"CSV file saved successfully to: {output_csv_path}\")\n",
    "else:\n",
    "    print(\"No tables found in the PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eb8d7e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred: sequence index out of range\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\sample1.pdf\"\n",
    "\n",
    "try:\n",
    "    # Extract tables from the PDF file using 'stream' flavor\n",
    "    tables = camelot.read_pdf(pdf_path, flavor='stream', pages='1,2')\n",
    "\n",
    "    # Check if any tables were found\n",
    "    if tables:\n",
    "        for i, table in enumerate(tables):\n",
    "            # Adjust the headers properly\n",
    "            df = table.df\n",
    "            new_header = df.iloc[1]  # take the second row for headers\n",
    "            df = df[2:]  # take the data less the header row\n",
    "            df.columns = new_header  # set the header row as the df header\n",
    "            \n",
    "            # Print the DataFrame to inspect\n",
    "            print(f\"Extracted DataFrame from Table {i + 1}:\")\n",
    "            print(df)\n",
    "\n",
    "            # Save each DataFrame to a CSV file with UTF-8 encoding\n",
    "            output_csv_path = rf\"C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table{i + 1}.csv\"\n",
    "            df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')  # Ensure UTF-8 encoding\n",
    "\n",
    "            print(f\"CSV file for Table {i + 1} saved successfully to: {output_csv_path}\")\n",
    "    else:\n",
    "        print(\"No tables found in the specified pages of the PDF.\")\n",
    "\n",
    "except IndexError as e:\n",
    "    print(f\"IndexError occurred: {e}\")\n",
    "    # Additional logging or debugging steps can be added here\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # Additional logging or debugging steps can be added here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d5ba9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytesseract in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (0.3.10)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (9.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "565a834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (1.24.7)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (9.2.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (0.3.10)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.6 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from PyMuPDF) (1.24.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF Pillow pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05e46b28",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 740] The requested operation requires elevation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13692\\110934388.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Example usage:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\ASUS\\Desktop\\text_extract\\sample1.pdf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mextracted_tables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_tables_from_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextracted_tables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13692\\110934388.py\u001b[0m in \u001b[0;36mextract_tables_from_pdf\u001b[1;34m(pdf_file)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mpix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pixmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Simple pattern to detect tables (adjust as per your needs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13692\\110934388.py\u001b[0m in \u001b[0;36mextract_text_from_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Function to extract text using OCR from an image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_text_from_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Function to convert PDF to images and extract text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mimage_to_string\u001b[1;34m(image, lang, config, nice, output_type, timeout)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m     return {\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBYTES\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDICT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[0mOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTRING\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrun_and_get_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m     }[output_type]()\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mrun_and_get_output\u001b[1;34m(image, extension, lang, config, nice, timeout, return_bytes)\u001b[0m\n\u001b[0;32m    286\u001b[0m         }\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mrun_tesseract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{kwargs['output_filename_base']}{extsep}{extension}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytesseract\\pytesseract.py\u001b[0m in \u001b[0;36mrun_tesseract\u001b[1;34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msubprocess_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 740] The requested operation requires elevation"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import re\n",
    "\n",
    "# Function to extract text using OCR from an image\n",
    "def extract_text_from_image(image):\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "# Function to convert PDF to images and extract text\n",
    "def extract_tables_from_pdf(pdf_file):\n",
    "    tables = []\n",
    "    doc = fitz.open(pdf_file)\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        pix = page.get_pixmap()\n",
    "        img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
    "        text = extract_text_from_image(img)\n",
    "\n",
    "        # Simple pattern to detect tables (adjust as per your needs)\n",
    "        if re.search(r'\\+[-+]+\\+', text):\n",
    "            tables.append(text)\n",
    "\n",
    "    doc.close()\n",
    "    return tables\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = r'C:\\Users\\ASUS\\Desktop\\text_extract\\sample1.pdf'\n",
    "extracted_tables = extract_tables_from_pdf(pdf_file)\n",
    "\n",
    "for idx, table in enumerate(extracted_tables, start=1):\n",
    "    print(f\"Table {idx}:\")\n",
    "    print(table)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736577e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c0f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c732e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e533b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f830c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404efd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a70f851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c728b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b93fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4dc26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0737471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "eae98d97",
   "metadata": {},
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf\"\n",
    "\n",
    "# Extract tables from the PDF file\n",
    "tables = camelot.read_pdf(pdf_path, pages='1', flavor='stream')\n",
    "\n",
    "# Check if any tables were found\n",
    "if not tables:\n",
    "    print(\"No tables found in the PDF.\")\n",
    "else:\n",
    "    # Assuming the table we need is the first one found\n",
    "    df = tables[0].df\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(\"Extracted DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table.csv\"\n",
    "    df.to_csv(output_csv_path, index=False, header=False)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "170a3bb4",
   "metadata": {},
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf\"\n",
    "\n",
    "# Extract tables from the PDF file\n",
    "tables = camelot.read_pdf(pdf_path, pages='1', flavor='stream')\n",
    "\n",
    "# Check if any tables were found\n",
    "if tables.n > 0:\n",
    "    # Assuming the table we need is the first one found\n",
    "    df = tables[0].df\n",
    "\n",
    "    # Print the raw DataFrame to diagnose issues\n",
    "    print(\"Extracted DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Clean and format the DataFrame\n",
    "    # Assuming the first row is the header\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.drop(0).reset_index(drop=True)\n",
    "\n",
    "    # Strip any extra whitespace from column names\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = r\"C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_table.csv\"\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    # Display the formatted DataFrame\n",
    "    print(\"Formatted DataFrame:\")\n",
    "    print(df)\n",
    "\n",
    "    # Get the value of \"Participants\" based on \"Disability Category\"\n",
    "    category = \"Blind\"  # Example category\n",
    "    participants_value = df.loc[df['Disability Category'] == category, 'Participants'].values[0]\n",
    "\n",
    "    # Display the result\n",
    "    print(f\"Participants in category '{category}': {participants_value}\")\n",
    "\n",
    "else:\n",
    "    print(\"No tables found in the PDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1c9139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (20240706)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.2-py3-none-any.whl (58 kB)\n",
      "     -------------------------------------- 58.0/58.0 kB 508.3 kB/s eta 0:00:00\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (37.0.1)\n",
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting pypdfium2>=4.18.0\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "     ---------------------------------------- 2.9/2.9 MB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfplumber) (9.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
      "Installing collected packages: pypdfium2, pdf2image, pdfminer.six, pdfplumber\n",
      "  Attempting uninstall: pdfminer.six\n",
      "    Found existing installation: pdfminer.six 20240706\n",
      "    Uninstalling pdfminer.six-20240706:\n",
      "      Successfully uninstalled pdfminer.six-20240706\n",
      "Successfully installed pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.2 pypdfium2-4.30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script pypdfium2.exe is installed in 'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pdfplumber.exe is installed in 'C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six pdfplumber pdf2image beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd55178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pdfplumber in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (0.11.2)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfplumber) (9.2.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (37.0.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eada07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def pdf_to_html(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_tables_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    tables_data = []\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        table_data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td', 'th'])\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            table_data.append(cols)\n",
    "        tables_data.append(pd.DataFrame(table_data))\n",
    "    \n",
    "    return tables_data\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    # Convert PDF to HTML text\n",
    "    pdf_text = pdf_to_html(pdf_path)\n",
    "    \n",
    "    # Convert HTML text to HTML format\n",
    "    html_content = f\"<html><body>{pdf_text}</body></html>\"\n",
    "    \n",
    "    # Extract tables from HTML\n",
    "    tables = extract_tables_from_html(html_content)\n",
    "    \n",
    "    return tables\n",
    "\n",
    "# Example usage\n",
    "pdf_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf'\n",
    "tables = process_pdf(pdf_path)\n",
    "\n",
    "# Display or save the extracted tables\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i+1}\")\n",
    "    print(table)\n",
    "    table.to_csv(f'table_{i+1}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b21eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def pdf_to_html(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_tables_from_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    tables_data = []\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        table_data = []\n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td', 'th'])\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            table_data.append(cols)\n",
    "        tables_data.append(pd.DataFrame(table_data))\n",
    "    \n",
    "    return tables_data\n",
    "\n",
    "def process_pdf(pdf_path, output_dir):\n",
    "    # Convert PDF to HTML text\n",
    "    pdf_text = pdf_to_html(pdf_path)\n",
    "    \n",
    "    # Convert HTML text to HTML format\n",
    "    html_content = f\"<html><body>{pdf_text}</body></html>\"\n",
    "    \n",
    "    # Extract tables from HTML\n",
    "    tables = extract_tables_from_html(html_content)\n",
    "    \n",
    "    # Save tables as CSV files\n",
    "    for i, table in enumerate(tables):\n",
    "        csv_path = os.path.join(output_dir, f'table_{i+1}.csv')\n",
    "        table.to_csv(csv_path, index=False)\n",
    "        print(f\"Table {i+1} saved as {csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf'\n",
    "output_directory = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output'  # Specify your desired output directory\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Process PDF and save tables as CSV\n",
    "process_pdf(pdf_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0062388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Content: <!DOCTYPE html>\n",
      "<!-- Created by pdf2htmlEX (https://github.com/pdf2htmlEX/pdf2htmlEX) -->\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta name=\"generator\" content=\"pdf2htmlEX\"/>\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"/>\n",
      "<style type=\"text/css\">\n",
      "/*! \n",
      " * Base CSS for pdf2htmlEX\n",
      " * Copyright 2012,2013 Lu Wang <coolwanglu@gmail.com> \n",
      " * https://github.com/pdf2htmlEX/pdf2htmlEX/blob/master/share/LICENSE\n",
      " */#sidebar{position:absolute;top:0;left:0;b\n",
      "Found 0 data rows\n",
      "Extracted Data from HTML: []\n",
      "Extracted data saved as C:\\Users\\ASUS\\Desktop\\text_extract\\output\\extracted_data.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_data_from_html(html_file):\n",
    "    try:\n",
    "        with open(html_file, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: HTML file '{html_file}' not found.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Adjust this selector based on your HTML structure\n",
    "    data_rows = soup.find_all('div', class_='table-row')  # Example class name, adjust as per your HTML\n",
    "\n",
    "    extracted_data = []\n",
    "    for row in data_rows:\n",
    "        # Adjust this selector based on how your data is structured within each row\n",
    "        cells = row.find_all('div', class_='cell')  # Example class name, adjust as per your HTML\n",
    "        row_data = [cell.text.strip() for cell in cells]\n",
    "        extracted_data.append(row_data)\n",
    "\n",
    "    # Debugging output\n",
    "    print(f\"HTML Content: {html_content[:500]}\")  # Print a snippet of HTML content\n",
    "    print(f\"Found {len(data_rows)} data rows\")\n",
    "    for i, row_data in enumerate(extracted_data):\n",
    "        print(f\"Row {i+1} data: {row_data}\")\n",
    "\n",
    "    print(f\"Extracted Data from HTML: {extracted_data}\")\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "def save_data_to_csv(data, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert to DataFrame and save as CSV\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(output_dir, 'extracted_data.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Extracted data saved as {csv_path}\")\n",
    "\n",
    "# Example usage\n",
    "html_file = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.html'\n",
    "output_directory = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output'  # Specify the output directory for CSV file\n",
    "\n",
    "# Extract data from HTML file\n",
    "extracted_data = extract_data_from_html(html_file)\n",
    "\n",
    "# Save extracted data to CSV\n",
    "save_data_to_csv(extracted_data, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42653130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "html_file = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.html'\n",
    "excel_file = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output.xlsx' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0802d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def pdf_to_html(pdf_path, html_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    html = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        html += page.get_text(\"html\")\n",
    "    \n",
    "    with open(html_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "def extract_table_from_html(html_path):\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    \n",
    "    # Assuming the table you want is the first table in the HTML (you may need to adapt this)\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            for cell in cells:\n",
    "                print(cell.text.strip())  # Replace with your processing logic\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf'\n",
    "    html_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output.html'\n",
    "    \n",
    "    pdf_to_html(pdf_path, html_path)\n",
    "    extract_table_from_html(html_path)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1b65f87",
   "metadata": {},
   "source": [
    "import fitz  # PyMuPDF\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def pdf_to_html(pdf_path, html_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    html = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        html += page.get_text(\"html\")\n",
    "    \n",
    "    with open(html_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "def extract_table_from_html(html_path, output_csv):\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "        \n",
    "        # Print all tables to debug\n",
    "        tables = soup.find_all('table')\n",
    "        for table in tables:\n",
    "            print(\"Table:\")\n",
    "            print(table.prettify())\n",
    "        \n",
    "        # Example: Selecting the first table found\n",
    "        if tables:\n",
    "            table = tables[0]  # Change the index as needed to select the correct table\n",
    "            rows = table.find_all('tr')\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                cells = row.find_all(['th', 'td'])\n",
    "                data.append([cell.text.strip() for cell in cells])\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            if not df.empty:\n",
    "                print(\"DataFrame:\")\n",
    "                print(df)  # Print DataFrame\n",
    "                \n",
    "                df.to_csv(output_csv, index=False, header=False, encoding='utf-8')\n",
    "                print(f\"Table data saved to {output_csv}.\")\n",
    "            else:\n",
    "                print(\"DataFrame is empty. No data to print or save.\")\n",
    "        else:\n",
    "            print(\"No tables found in the HTML.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting table: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf'\n",
    "    html_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output\\1.html'\n",
    "    output_csv = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output\\2.csv'\n",
    "    \n",
    "    pdf_to_html(pdf_path, html_path)\n",
    "    extract_table_from_html(html_path, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef6752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7d479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def extract_tables_from_text(pdf_text, output_csv):\n",
    "    # Split text into tables based on identified delimiters\n",
    "    tables = pdf_text.split('\\n\\n')  # Split by double newlines assuming each table is separated by blank lines\n",
    "    \n",
    "    if tables:\n",
    "        data = []\n",
    "        for table in tables:\n",
    "            rows = table.split('\\n')\n",
    "            cleaned_rows = [row.strip() for row in rows if row.strip()]\n",
    "            data.append(cleaned_rows)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        if not df.empty:\n",
    "            print(\"DataFrame:\")\n",
    "            print(df)  # Print DataFrame\n",
    "            \n",
    "            df.to_csv(output_csv, index=False, header=False, encoding='utf-8')\n",
    "            print(f\"Table data saved to {output_csv}.\")\n",
    "        else:\n",
    "            print(\"DataFrame is empty. No data to print or save.\")\n",
    "    else:\n",
    "        print(\"No tables found in the PDF text.\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf'\n",
    "    output_csv = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output\\2.csv'\n",
    "    \n",
    "    pdf_text = pdf_to_text(pdf_path)\n",
    "    extract_tables_from_text(pdf_text, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01b92846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "        Column_1                             Column_2    Column_3  Column_4  \\\n",
      "0  Example table  This is an example of a data table.  Disability  Category   \n",
      "\n",
      "       Column_5 Column_6   Column_7 Column_8     Column_9   Column_10  ...  \\\n",
      "0  Participants  Ballots  Completed  Ballots  Incomplete/  Terminated  ...   \n",
      "\n",
      "      Column_26      Column_27        Column_28  Column_29 Column_30  \\\n",
      "0  (97.7%, n=3)  1716 sec, n=3  (1934 sec, n=2)  Dexterity         5   \n",
      "\n",
      "  Column_31 Column_32   Column_33        Column_34 Column_35  \n",
      "0         4         1  98.3%, n=4  1672.1 sec, n=4  Mobility  \n",
      "\n",
      "[1 rows x 35 columns]\n",
      "Table data saved to C:\\Users\\ASUS\\Desktop\\text_extract\\output\\2.csv.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def extract_tables_from_text(pdf_text, output_csv):\n",
    "    # Split text into tables based on identified delimiters\n",
    "    tables = pdf_text.split('\\n\\n\\n')  # Split by triple newlines assuming each table is separated by two blank lines\n",
    "    \n",
    "    if tables:\n",
    "        data = []\n",
    "        for table in tables:\n",
    "            rows = table.strip().split('\\n')\n",
    "            cleaned_rows = [row.strip() for row in rows if row.strip()]\n",
    "            data.append(cleaned_rows)\n",
    "        \n",
    "        # Determine max number of columns in any table (this assumes all tables have the same structure)\n",
    "        max_columns = max(len(row) for row in data[0]) if data else 0\n",
    "        \n",
    "        # Create a list of dictionaries for each row in the table\n",
    "        table_data = []\n",
    "        for row in data:\n",
    "            table_row = {}\n",
    "            for i in range(max_columns):\n",
    "                if i < len(row):\n",
    "                    table_row[f'Column_{i+1}'] = row[i]\n",
    "                else:\n",
    "                    table_row[f'Column_{i+1}'] = ''\n",
    "            table_data.append(table_row)\n",
    "        \n",
    "        df = pd.DataFrame(table_data)\n",
    "        \n",
    "        if not df.empty:\n",
    "            print(\"DataFrame:\")\n",
    "            print(df)  # Print DataFrame\n",
    "            \n",
    "            df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "            print(f\"Table data saved to {output_csv}.\")\n",
    "        else:\n",
    "            print(\"DataFrame is empty. No data to print or save.\")\n",
    "    else:\n",
    "        print(\"No tables found in the PDF text.\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r'C:\\Users\\ASUS\\Desktop\\text_extract\\table.pdf'\n",
    "    output_csv = r'C:\\Users\\ASUS\\Desktop\\text_extract\\output\\2.csv'\n",
    "    \n",
    "    pdf_text = pdf_to_text(pdf_path)\n",
    "    extract_tables_from_text(pdf_text, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c7ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c3f849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e179ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
