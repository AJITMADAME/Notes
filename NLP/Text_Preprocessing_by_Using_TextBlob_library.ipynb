{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TextBlob: Simplified Text Processing\n",
        "\n",
        "TextBlob is apython library for processing textual data. It provide a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation and more.\n",
        "\n",
        "TextBlob stands on the giant shoulders of NLTK and pattern, and plays nicely with both.\n",
        "\n",
        "### Features:\n",
        "\n",
        "- Noun phrase extraction\n",
        "- Part-of-speech tagging\n",
        "- Sentiment Analysis\n",
        "- Classification(Naive Bayes, Decision Tree)\n",
        "- Language translation and detection powered by Google Translate\n",
        "- Tokenization (Splitting text into words and sentences)\n",
        "- Word and phrase frequencies\n",
        "- Parsing\n",
        "- n-grams\n",
        "- Word inflection (pluralization and singularization) and lemmatization\n",
        "- Spelling correction\n",
        "- Add new models or language through extensions\n",
        "- WordNet integration"
      ],
      "metadata": {
        "id": "IdJ9KrGlbKsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "egtSUGakbR0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbs2kgZZbIIO",
        "outputId": "0f44c80d-9c12-4976-f2ff-e44afc297afa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.8/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob) (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a TextBlob\n",
        "\n",
        "for first import"
      ],
      "metadata": {
        "id": "tfsuaYSdbaSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "_DjjfJDmbILH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our first TextBlob"
      ],
      "metadata": {
        "id": "hUzAzpAxbica"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki = TextBlob(\"I Love Natural Language Processing, not you!\")"
      ],
      "metadata": {
        "id": "KmWRbXfPbINi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part-of-speech(POS) Tagging\n",
        "Parts-of-speech tags can be accessed through the tags property."
      ],
      "metadata": {
        "id": "Nap6HvjCbpVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ghnhlsGcHA_",
        "outputId": "726aa988-49b8-4a61-eb96-2c7e9a9fb2df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mykNQlTocOp6",
        "outputId": "1e8b8f5b-a7e7-4685-d2c0-6116c02687dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki.tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd1IYUH7bIQL",
        "outputId": "2e34aaef-f63a-4a1f-a7f6-933ce95a1204"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('Love', 'VBP'),\n",
              " ('Natural', 'JJ'),\n",
              " ('Language', 'NNP'),\n",
              " ('Processing', 'NNP'),\n",
              " ('not', 'RB'),\n",
              " ('you', 'PRP')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Noun Phrase Extraction\n",
        "\n",
        "Similarly, noun phrases are accessed through the noun_phrase property"
      ],
      "metadata": {
        "id": "kb9-JpUpcmTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki.noun_phrases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyDny6LZcKRx",
        "outputId": "a7679a93-94db-4640-8013-29b08939d9ba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['love', 'language processing'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "The sentiment propert returns a named tuple of the form Sentiments(polarity, subjective). The polarity score is a float within the range [-1.0,1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
      ],
      "metadata": {
        "id": "fDM-rgFpdCIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
        "testimonial.sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTZTYsA_bIVp",
        "outputId": "4e11268b-94db-4adc-8ef7-e21c8c029cc5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testimonial.sentiment.subjectivity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OcvFZRobIYP",
        "outputId": "10afff33-28f2-47f7-86d7-c81a11247c63"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4357142857142857"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "Ax32nOROeAjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zen = TextBlob(\"Data is a new fuel. \"\n",
        "               \"Explicit is better than implicit. \"\n",
        "               \"Simple is better than complex. \")\n",
        "\n",
        "zen.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmhP61lXbIbH",
        "outputId": "ad87cc19-635e-4ee9-b757-d4007dd0e9b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Data', 'is', 'a', 'new', 'fuel', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zen.sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXw914lkbId2",
        "outputId": "862248dd-8670-4bc2-b73e-5d5ea64a517c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence(\"Data is a new fuel.\"),\n",
              " Sentence(\"Explicit is better than implicit.\"),\n",
              " Sentence(\"Simple is better than complex.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentences objects have the same properties and methods as TextBlobs."
      ],
      "metadata": {
        "id": "A9Jzmmhcek2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in zen.sentences:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE5-pL-8bIgf",
        "outputId": "eb7028aa-9555-4a7b-81f9-682b93cc3398"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is a new fuel.\n",
            "Explicit is better than implicit.\n",
            "Simple is better than complex.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word inflection and Lemmatization\n",
        "\n",
        "Each word in the TexBlob.words or Sentences.words is a Word objects(a subclass of unicode) with useful methods, e.g. for word inflection."
      ],
      "metadata": {
        "id": "2uM-GPNWe6E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = TextBlob(\"Use 4 spaces per indentation level\")\n",
        "\n",
        "sentence.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85a7lhUKbIjN",
        "outputId": "0e529bf1-d561-4e62-8fed-2321fbdcd108"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.words[2].singularize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OlveI3WWbImC",
        "outputId": "f1800adc-af46-4230-d2ad-65079e1c2a63"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'space'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.words[0].pluralize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nSV_7TqPbIo3",
        "outputId": "81ee2da4-0e31-4b12-c8b6-6ba629c56776"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Uses'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words can be lemmanized just by calling the lemmatize method."
      ],
      "metadata": {
        "id": "IPPei7S4gdWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "\n",
        "q = Word('lions')\n",
        "q.lemmatize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8Y5TGXbgbIrs",
        "outputId": "f9c49a2a-7bc7-419a-f9f8-cde20ef73787"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = Word(\"went\")\n",
        "q.lemmatize('v')  # Pass in WordNet part of speech"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LqefF1PnbIum",
        "outputId": "5e26db3e-4375-42f5-e97a-aa9013f43c1d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WordNet Integration\n",
        "\n",
        "You can access the synets for a Word via the synsets property or the get_synsets methods optionally passing in a parts-of-speech.\n",
        "\n",
        "### WordNet\n",
        "\n",
        "WordNet is a lexical database that is dictionary for the English language, it is specifically for the natural language processing.\n",
        "\n",
        "### Synset:\n",
        "It is a special kind of a simple interface that is present in the NLTK for look up words in WordNet. Synset instances are the groupings of synonymous that express the same type of concept. Some words have only one synset and some have several.\n",
        "\n",
        "You can access the definitions for each synset via the definitions property or the define() method, which can also take an optional part-of-speech(POS) argument."
      ],
      "metadata": {
        "id": "69sAGwS4hIx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Word(\"length\").definitions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdFHTSOlbI23",
        "outputId": "2de6ccf1-5bc4-4e58-f84a-79e67015aba7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the linear extent in space from one end to the other; the longest dimension of something that is fixed in place',\n",
              " 'continuance in time',\n",
              " 'the property of being the extent of something from beginning to end',\n",
              " 'size of the gap between two places',\n",
              " 'a section of something that is long and narrow']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create synsets directly."
      ],
      "metadata": {
        "id": "rjFpPqSyirav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob.wordnet import Synset\n",
        "octopus = Synset('octopus.n.02')\n",
        "shrimp = Synset('shrimp.n.03')\n",
        "octopus.path_similarity(shrimp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7wnlzv4bI51",
        "outputId": "f754e1fc-26c3-49f1-cb6b-d5d774d3a2ff"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordLists\n",
        "\n",
        "A WordList is just the python list additionaly methods.\n",
        "\n",
        "WordLists will find it out the words which are in the sentence and ignore the spaces in between them."
      ],
      "metadata": {
        "id": "DH9HO6yHjIbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animals = TextBlob(\"cow sheep octopus\")\n",
        "animals.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b896QNXbI8u",
        "outputId": "8f056844-1a21-4b69-a1d3-f80e106489d6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['cow', 'sheep', 'octopus'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals.words.pluralize()  # It'll pluralize the words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ1CotMCbI_l",
        "outputId": "61ecd3c5-97ae-4981-fe65-eeee955e2fc6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['kine', 'sheep', 'octopodes'])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spelling Correction\n",
        "For correcting the words you can use correct() method to attempt spelling correction.\n"
      ],
      "metadata": {
        "id": "nHPOeTwyjsel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = TextBlob(\"Can you pronounce czechuslovakia?\")\n",
        "print(g.correct())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzJx3JxPbJCc",
        "outputId": "5c5df751-f41c-41fd-c2c3-08186a8b3c25"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An you pronounce czechoslovakia?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word objects have a spellcheck() Word.spellcheck(). this method that returns a list of (word, confidence) tuple with spelling suggestions."
      ],
      "metadata": {
        "id": "ODJQOCHckMwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "k = Word(\"longituode\")\n",
        "k.spellcheck()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91w7AFu8bJFV",
        "outputId": "557e59b9-a19e-4540-9fd5-cceae6e91eaf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('longitude', 1.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This spelling correction is based on the Peter Norvig's \"How to Write a Spelling Corrector\". as implemented in the pattern library, it is about 70% accurate.\n",
        "\n",
        "### Get Word and Noun Phrase Frequencies\n",
        "\n",
        "There are two ways to get the frequencies of a word or noun phrase in the TextBlob.\n",
        "\n",
        "The first one is through the word_counts dictionary."
      ],
      "metadata": {
        "id": "DoP3c8fikv5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = TextBlob('She sales sea shells at the sea shore.')\n",
        "\n",
        "sent.word_counts['sea']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyETQDZRbJH8",
        "outputId": "1d518795-193a-4dfe-f66e-e3c20f3cb5b9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you access the frequencies this way, the search will not be case sensitive, and words that are not found will have a frequency of 0.\n",
        "\n",
        "The second wat is to use the count() method."
      ],
      "metadata": {
        "id": "yA4ac8bBlqU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent.words.count('sea')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyqJhK5qbJK4",
        "outputId": "f533d60f-3787-4357-b715-3c99a08fd6c8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can specify whether or not the search should be case-sensitive (default is False)."
      ],
      "metadata": {
        "id": "vPRWtIr4mGC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent.words.count('Sea', case_sensitive=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DIjOhc6bJNw",
        "outputId": "729c33f7-004b-4198-ae33-0c77804f2a34"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example we have give 'Sea' and ofcourse \"Sea\" is not available in the sentence,'Sea' is not available in the sentence,'Sea' is available in the sentence but in lowercase because of that it given 0 as result.\n",
        "\n",
        "Each of these methods can also be used with noun phrases."
      ],
      "metadata": {
        "id": "vAocdEtymbRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent.noun_phrases.count('sea')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RtqDwntbJQY",
        "outputId": "d54a7e88-3b87-4857-fc36-7c34c626f49c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation and Language Detection\n",
        "\n",
        "TextBlob can be translated between languages."
      ],
      "metadata": {
        "id": "BY5AspH-nYoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(u'Something is better than nothing.')\n",
        "blob.translate(from_lang='en', to='hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkRm_W98bJTY",
        "outputId": "d69ecd2d-9ce4-4a74-a9a3-e0b18bb0701d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"कुछ नहीं से कुछ भला।\")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"hello\")\n",
        "blob.translate(from_lang='en', to='hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdZM1iGhbJbt",
        "outputId": "743f1572-d3ca-4da9-8caf-c21acf45899b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"नमस्ते\")"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If no source language is specified. TextBlob will attempt to detect the language. You can specify the source language explicity, like so. Raises TranslatorError if the TextBlob cannot be translate into the requested language or NotTranslated if the translated result is the same as the input string."
      ],
      "metadata": {
        "id": "SA4bM4CbqLDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chinese_blob = TextBlob(u\"有总比没有好\")\n",
        "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7rDWhXfbJha",
        "outputId": "e017938a-e412-4f71-f8b4-c13db9350ec2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"There is always better than not\")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also attempt to detect a TextBlob's language using TextBlob.detect_language()."
      ],
      "metadata": {
        "id": "gqyP6hSSrSK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run this cell after when you have good internet connection\n",
        "\n",
        "d = TextBlob(\"कुछ नहीं से कुछ भला\")\n",
        "\n",
        "d.detect_language()"
      ],
      "metadata": {
        "id": "IiMmA91cvUnv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zR896mFMtmma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing \n",
        "\n",
        "use the parse() method to parse the text."
      ],
      "metadata": {
        "id": "0Y_9qvrmvkAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = TextBlob(\"And now for something completely different.\")\n",
        "print(b.parse())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVuncxkJtmpL",
        "outputId": "8db76a85-3984-4b18-e88e-102de2916492"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And/CC/O/O now/RB/B-ADVP/O for/IN/B-PP/B-PNP something/NN/B-NP/I-PNP completely/RB/B-ADJP/O different/JJ/I-ADJP/O ././O/O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### translate Are Like Python Strings!\n",
        "\n",
        "You can use Python's substring syntax"
      ],
      "metadata": {
        "id": "kNHtTEtZv6zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zen[0:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaEyosWVtmrz",
        "outputId": "fd25c437-aeac-4f33-e786-dade8f68fd44"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"Data is a new f\")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see use it as common string method."
      ],
      "metadata": {
        "id": "SvOHbmJZwHuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zen.upper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EOMuiDPtmup",
        "outputId": "433c856d-1df2-4d3c-c775-69d78885fd52"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"DATA IS A NEW FUEL. EXPLICIT IS BETTER THAN IMPLICIT. SIMPLE IS BETTER THAN COMPLEX. \")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zen.find('than')  # It shows that 'than' word stars from 39th place."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkev82Wwtmxg",
        "outputId": "93a19d35-e54e-4270-e41d-6438b187bb27"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can make comparisons between TextBlobs and strings."
      ],
      "metadata": {
        "id": "bRMSo55cwbtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_blob = TextBlob('apple')\n",
        "s_blob = TextBlob('samsung')\n",
        "\n",
        "a_blob < s_blob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iDs0bs2tmzy",
        "outputId": "17a8b47b-b2e9-4581-db58-a82aa19ec9ff"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a_blob =='apple'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wUZrRjntm2k",
        "outputId": "e408e574-e2e6-481b-9508-a1e0110302ab"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can concatenate and interpolate TextBlobs and strigs."
      ],
      "metadata": {
        "id": "TKbjyu04w3ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_blob + ' and ' + s_blob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m76PgLltm5d",
        "outputId": "166c620f-bb3e-4d15-bebc-de5f14e30329"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"apple and samsung\")"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"{0} and {1}\".format(a_blob,s_blob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dYk0U9-sxB0X",
        "outputId": "8d4b9692-2c85-4e5f-ea95-8dc555524019"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'apple and samsung'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams\n",
        "\n",
        "TextBlob.ngrams() methods returns a list of tuple of n successive words."
      ],
      "metadata": {
        "id": "aKiF2VNPxTa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"Now is better than never.\")\n",
        "blob.ngrams(n=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvG7J-IWxB3E",
        "outputId": "cc9d8cba-b50e-4a02-f095-5710ea90a0dc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['Now', 'is', 'better']),\n",
              " WordList(['is', 'better', 'than']),\n",
              " WordList(['better', 'than', 'never'])]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"Now is better than never.\")\n",
        "blob.ngrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxSamiHWxB6g",
        "outputId": "f0752b0d-bb1c-4ea9-9436-702c8ba2d8f2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['Now', 'is']),\n",
              " WordList(['is', 'better']),\n",
              " WordList(['better', 'than']),\n",
              " WordList(['than', 'never'])]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Start and End Indicates of Sentences\n",
        "\n",
        "use sentence.start and sentence.end to get the indices where a sentence starts and ends whitin a TextBlob."
      ],
      "metadata": {
        "id": "CNXgTcHfxuCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in zen.sentences:\n",
        "  print(k)\n",
        "  print(\"---- Starts at index {}, Ends at index {}\".format(k.start, k.end))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXKyNrWoxB8v",
        "outputId": "0121d1e3-6b88-4271-b1e9-4a7d5f61e1eb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is a new fuel.\n",
            "---- Starts at index 0, Ends at index 19\n",
            "Explicit is better than implicit.\n",
            "---- Starts at index 20, Ends at index 53\n",
            "Simple is better than complex.\n",
            "---- Starts at index 54, Ends at index 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets Starty building the Text Classification system\n",
        "\n",
        "The textblob.classifiers module makes it simple to create custom classifiers.\n",
        "\n",
        "As an examples, lets create a custom sentiment analyser.\n",
        "\n",
        "### Loading data and Creating a Classifier\n",
        "First we'll create some training and test data."
      ],
      "metadata": {
        "id": "oZ65HAvtyYzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = [\n",
        "    (\"I love this sandwich.\",'pos'),\n",
        "    ('this is an amazing place!','pos'),\n",
        "    ('I feel very good about these beers.','pop'),\n",
        "    ('this is my best work.','pos'),\n",
        "    ('what an awesome view','pos'),\n",
        "    ('I do not like this restaurant','neg'),\n",
        "    ('I am tried of this stuff.','neg'),\n",
        "    (\"I can't deal with this\",'neg'),\n",
        "    ('he is my sworn enemy!','neg'),\n",
        "    ('my boss is horrible.','neg')\n",
        "]\n",
        "\n",
        "test = [\n",
        "    ('the beer was good.','pos'),\n",
        "    ('I do not enjoy my job','neg'),\n",
        "    (\"I ain't feeling dandy today.\",'neg'),\n",
        "    (\"I feel amazing!\",'pos'),\n",
        "    ('Gary is a friend of mine.','pos'),\n",
        "    (\"I can't believe I'm doing this.\",'neg')\n",
        "]"
      ],
      "metadata": {
        "id": "vm_sBegcxB_j"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll create a Naive Bayes classifier, passing the training data into the constructor."
      ],
      "metadata": {
        "id": "2PczxVZq1JX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "cl = NaiveBayesClassifier(train)"
      ],
      "metadata": {
        "id": "NrCHfwdpxCB_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifying Text\n",
        "\n",
        "Call the classify(text) method to use the classifier"
      ],
      "metadata": {
        "id": "TJoisQJu1lEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl.classify(\"This is an amazing library!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ea4mxgBExCHm",
        "outputId": "38c5edaa-f47e-41a4-bf04-5f6773e44e5b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can get the label probability distribution with the prob_classify(text) method."
      ],
      "metadata": {
        "id": "uZ5JOFs414Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prob_dist = cl.prob_classify(\"I am suffering from cough and cold.\")\n",
        "prob_dist.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pzx57Z87xCKp",
        "outputId": "da5b5e19-a5a3-48a6-a595-3ecfc42b8a85"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round(prob_dist.prob(\"neg\"), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfWDx2kjxCNP",
        "outputId": "655d581f-7fb7-4de5-e0aa-ff6a2b7a53e3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.91"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round(prob_dist.prob(\"pos\"), 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj2NWnE5xCQY",
        "outputId": "042311db-b04f-44b9-dab3-b6116f9551fc"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifying TextBlobs\n",
        "\n",
        "Anothet way to classify is to pass a classifer into the constructor of TextBlob and call its classify()method."
      ],
      "metadata": {
        "id": "ug1ssQM82lhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"Alcohol is good. But the hangover is horrible.\", classifier=cl)\n",
        "blob.classify()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vEDznLo9xCS5",
        "outputId": "13c4f432-4f7e-4ec4-ad86-84c8bbb2d986"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in blob.sentences:\n",
        "  print(b)\n",
        "  print(b.classify())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PprzbT46xCVs",
        "outputId": "bbaaac5c-fbee-47fd-940e-0b43f5a69d8c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alcohol is good.\n",
            "pos\n",
            "But the hangover is horrible.\n",
            "neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Classifiers"
      ],
      "metadata": {
        "id": "Sqly8wNp3qHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl.accuracy(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_I0kiNixCYw",
        "outputId": "f2725ec2-c955-49c8-fdb9-ed7d8d0eac4a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the show_informative_features() method to display a listing of the most informative features."
      ],
      "metadata": {
        "id": "ahxFL4ZW30Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl.show_informative_features(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGoHCdGZxCbQ",
        "outputId": "678f52f2-e854-48e8-bf56-5c5c0d1d0dd6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             contains(I) = True              pop : pos    =      2.5 : 1.0\n",
            "          contains(this) = False             pop : pos    =      2.5 : 1.0\n",
            "            contains(an) = False             neg : pos    =      1.8 : 1.0\n",
            "             contains(I) = False             pos : neg    =      1.7 : 1.0\n",
            "            contains(is) = False             pop : pos    =      1.5 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating classifiers with new Data."
      ],
      "metadata": {
        "id": "KTe1E8sO4HQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = [(\"She is my best friend.\",'pos'),\n",
        "            (\"I'm happy to have a new friend.\",'pos'),\n",
        "            (\"Stay thirsty, my friend.\",'pop'),\n",
        "            (\"He ain't from around here.\",'neg')]\n",
        "\n",
        "cl.update(new_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ29hKvjxCes",
        "outputId": "97924210-13d0-48ce-8597-8981251ce1fb"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cl.accuracy(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvwUsnsRxCg8",
        "outputId": "a9a65a59-edc9-4439-b12c-7e0610d2c120"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "\n",
        "By default, the NaiveBayesClassifier users a simple feature extractor that indicates which words in the training set are containing in a document.\n",
        "\n",
        "For example, the sentence 'I love\" might have the features contains(love). True or contains(hate). False.\n",
        "\n",
        "You can override the this features extractor by writing your own. A feature extractor is simply a function with documents (the text to extract features from) as the first argument. The function may include a secondf argument, train_set(the training datasets) if necessary.\n",
        "\n",
        "The function should return a dictionary of features for documents."
      ],
      "metadata": {
        "id": "k1MANGr14zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def end_word_extractor(document):\n",
        "  tokens = document.split()\n",
        "  first_word, last_word = tokens[0], tokens[-1]\n",
        "  feats = {}\n",
        "  feats[\"first({0})\".format(first_word)] = True\n",
        "  feats[\"last({0})\".format(last_word)] = False\n",
        "  return feats"
      ],
      "metadata": {
        "id": "KaxCXQBmbJsQ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = end_word_extractor(\"I love\")"
      ],
      "metadata": {
        "id": "-sWdVEvTbJu8"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert features == {'last(love)': False, 'first(I)': True}"
      ],
      "metadata": {
        "id": "nC8q-0Jx6h4a"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then use the feature extractor in a classifier by passing it as the second argument of the constructor."
      ],
      "metadata": {
        "id": "PT3lbQoQ65vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl2 = NaiveBayesClassifier(test, feature_extractor=end_word_extractor)"
      ],
      "metadata": {
        "id": "Ft2tilF16h7D"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"I'm excited to try my new classifier.\", classifier=cl2)\n",
        "blob.classify()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4_To85tj6h9V",
        "outputId": "a4e5b484-788a-4811-9e0e-67fe88345791"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hlcg_RpB7cLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}